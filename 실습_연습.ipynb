{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020년 10월 22일(목)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 실습_00\n",
    "### <font color='red'> brier score를 구하고 예측모델 y_stupid의 혼동표와 분석보고서를 구하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0, 1, 1, 0])\n",
    "\n",
    "y_smart = np.array([0.1, 0.9, 0.8, 0.3])\n",
    "y_normal = np.array([0.4, 0.3, 0.6, 0.2])\n",
    "y_dummy = np.array([0.5, 0.5, 0.5, 0.5])\n",
    "y_stupid = np.array([0.9, 0.1, 0.7, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-8-1245a37dff45>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-1245a37dff45>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    for clf in [y_smart, y_normal, y_dummy, y_stupid]:\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for clf in [y_smart, y_normal, y_dummy, y_stupid]:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 실습_01\n",
    "    \n",
    "### <font color='red'> 실습 _01_A.  \n",
    "'encoded_data_TV_선호_scaled.xlsx' 파일을 불러와 훈련과 검증 8:2로 구분하여 혼동표와 분석보고서를 작성하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>성별_1</th>\n",
       "      <th>성별_2</th>\n",
       "      <th>지역_1</th>\n",
       "      <th>지역_2</th>\n",
       "      <th>지역_3</th>\n",
       "      <th>지역_4</th>\n",
       "      <th>지역_5</th>\n",
       "      <th>지역_6</th>\n",
       "      <th>지역_7</th>\n",
       "      <th>직업_1</th>\n",
       "      <th>...</th>\n",
       "      <th>드라마_genre_8</th>\n",
       "      <th>드라마_genre_9</th>\n",
       "      <th>드라마_genre_10</th>\n",
       "      <th>드라마_genre_11</th>\n",
       "      <th>드라마_genre_12</th>\n",
       "      <th>드라마_genre_13</th>\n",
       "      <th>드라마_genre_14</th>\n",
       "      <th>드라마_genre_15</th>\n",
       "      <th>드라마_genre_16</th>\n",
       "      <th>구매</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   성별_1  성별_2  지역_1  지역_2  지역_3  지역_4  지역_5  지역_6  지역_7  직업_1  ...  \\\n",
       "0     1     0     1     0     0     0     0     0     0     0  ...   \n",
       "1     1     0     1     0     0     0     0     0     0     0  ...   \n",
       "2     1     0     1     0     0     0     0     0     0     0  ...   \n",
       "3     1     0     1     0     0     0     0     0     0     0  ...   \n",
       "4     1     0     1     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "   드라마_genre_8  드라마_genre_9  드라마_genre_10  드라마_genre_11  드라마_genre_12  \\\n",
       "0          0.0         0.75           0.5      0.000000           0.0   \n",
       "1          0.0         0.00           0.0      0.333333           0.0   \n",
       "2          0.0         0.00           0.0      0.000000           0.5   \n",
       "3          0.0         0.00           0.0      0.000000           0.0   \n",
       "4          0.0         0.00           0.5      0.250000           0.0   \n",
       "\n",
       "   드라마_genre_13  드라마_genre_14  드라마_genre_15  드라마_genre_16  구매  \n",
       "0          0.25          0.00           0.0           0.0   1  \n",
       "1          0.00          0.00           0.0           0.0   0  \n",
       "2          0.00          0.25           0.0           0.0   1  \n",
       "3          0.00          0.00           0.0           0.0   0  \n",
       "4          0.00          0.00           0.0           0.0   0  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('../data/encoded_data_TV_선호_scaled.xlsx', index_col=0)   # 첫행은 건너띄고 파일 읽기\n",
    "df.head() # 처음 5줄 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '구매'변수를 제외하고 X로 features 객체화\n",
    "# target을 y로 객체화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 80%, 20%로 훈련과 검증데이터 구분하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "np.random.seed(1357)               \n",
    "# 알고리즘 객체화\n",
    "# 훈련데이터로 학습하기\n",
    "# 검증데이터로 예측하기\n",
    "# 검증데이터로 예측확률 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# 분석보고서 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# 혼동표 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> 실습 _01_B.\n",
    "'titanic' 파일을 불러와 훈련과 검증 8:2로 구분하여 혼동표와 분석보고서를 작성하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
       "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
       "       'alive', 'alone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex  sibsp     fare\n",
       "0         0       3    male      1   7.2500\n",
       "1         1       1  female      1  71.2833\n",
       "2         1       3  female      0   7.9250\n",
       "3         1       1  female      1  53.1000\n",
       "4         0       3    male      0   8.0500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.columns\n",
    "train = titanic[['survived', 'pclass', 'sex', 'sibsp',  'fare']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> 5차 교차검증으로 모델 정확도의 평균과 표준편차를 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'survived'변수 제외하고 features 생성하기\n",
    "# 종속변수 생성하여 객체화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성별(sex) 변수는 더미변수로 변환하여야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 'sex'변수 더미로 만들기\n",
    "# 'sex' 변수를 features에서 제거하기\n",
    "# 훈련과 검증데이터 8:2로 구분하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "# 알고리즘 소환\n",
    "# 교차검증 회수 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# 5 겹 교차검증의 모든 값\n",
    "# 5 겹 교차검증의 평균 확인하기\n",
    "# 5 겹 교차검증의 표준편차 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to data (학습하기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "# 검증데이터로 혼동표 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증데이터로 예측하기\n",
    "# 실제와 예측에 대한 분석보고서 작성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 실습_02\n",
    "### <font color='red'> 실습 _02_A.  \n",
    "\n",
    "- '소비자행태자료' 파일을 불러와 훈련과 검증 8:2로 구분하여 그리드서치로 혼동표와 분석보고서를 작성하라\n",
    "- 5겹 교차검증, 정확도기준, 훈련과 검증데이터의 target 비율은 동일하게\n",
    "- 하이퍼파라미터 n_neighbors=[3, 11, 17, 25], weights=['uniform','distance'], leaf_size= [4 ,6, 8, 10]를 적용하여 분석보고서를 작성하라\n",
    "- 최적모델의 파라미터를 나타내라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/encoded_data_TV_선호_scaled.xlsx', index_col=0)\n",
    "X = df.drop(['구매'], axis=1)\n",
    "y = df['구매']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=11) # 훈련과 검증데이터 8:2로 구분하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "# 알고리즘 소환\n",
    "# 교차검증 회수 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(n_neighbors=[11, 17, 25],\n",
    "                      weights=['uniform','distance'],\n",
    "                      leaf_size= [4 ,6])    # 최근접이웃, 거리비중, leaf내 자료 크기 하이퍼파라미터로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# knn으로 하이퍼파라미터 그리드서치 하기\n",
    "# 그리드서치 최적 모델(5겹 교차검증의 평균 정확도가 가장 큰 하이퍼파라미터를 가진 모델)로 학습하기, 학습 완료후 전체 데이터를 refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적 모델 확인하기\n",
    "# 최적 모델의 파라미터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# 검증데이터로 혼동표 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증데이터로 혼동표 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "# 예측하기\n",
    "# 분석보고서 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> 실습 _02_B. \n",
    "- iris 데이터의 예측을 수행하라. 혼동표를 추가로 작성하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=11) # 훈련과 검증데이터 8:2로 구분하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(n_neighbors=list(range(5,20, 2)),\n",
    "                  weights=['uniform','distance'],\n",
    "                  leaf_size= list(range(4, 10)),\n",
    "                  algorithm= ['ball_tree', 'kd_tree'],\n",
    "                  p= [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "model = KNeighborsClassifier() # 알고리즘 소환\n",
    "kf = KFold(n_splits=5, random_state=11) # 교차검증 회수 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# knn으로 하이퍼파라미터 무작위서치 하기\n",
    "# 무작위서치 최적 모델(5겹 교차검증의 평균 정확도가 가장 큰 하이퍼파라미터를 가진 모델)로 학습하기, 학습 완료후 전체 데이터를 refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-9957fef2980f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 최적 모델 확인하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 최적 모델의 파라미터 확인하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "# 최적 모델 확인하기\n",
    "grid.best_estimator_.get_params() # 최적 모델의 파라미터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "y_predict = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(grid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "무작위 서치를 늘리고 싶을 경우 n_iter=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "grid = RandomizedSearchCV(model, hyperparameters, cv=kf, n_iter=50, verbose=True) # knn으로 하이퍼파라미터 무작위서치 하기\n",
    "grid.fit(X_train, y_train) # 무작위서치 최적 모델(5겹 교차검증의 평균 정확도가 가장 큰 하이퍼파라미터를 가진 모델)로 학습하기, 학습 완료후 전체 데이터를 refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-709db625a4a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m     \u001b[1;31m# 최적 모델 확인하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 최적 모델의 파라미터 확인하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "grid.best_estimator_     # 최적 모델 확인하기\n",
    "grid.best_estimator_.get_params() # 최적 모델의 파라미터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-64507ce590ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "y_predict = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 실습_03 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "numeric_features = ['age', 'fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 구성하기\n",
    "# 훈련과 검증데이터 구분하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터전처리와 로지스틱 알고리즘 파이프라인구성\n",
    "\n",
    "#학습하기\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Intermission\n",
    "###  오렌지색(X, y의 평균)을 분류 예측하고 구분선을 시각화한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(200, 2, centers=2, random_state=2, cluster_std=5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='coolwarm')\n",
    "plt.scatter(X[:, 0].mean(), X[:, 1].mean(), color = '#ff7f0e', s=200, marker='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmean = np.array([X[:, 0].mean(), X[:, 1].mean()])        # 가로세록축의 평균을 구한다\n",
    "Xmean.shape\n",
    "Xnew = Xmean[np.newaxis, :]     # 차원을 확대\n",
    "Xnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "MLA = [KNeighborsClassifier(), GaussianNB(),  LogisticRegression(), \n",
    "       DecisionTreeClassifier(), RandomForestClassifier(),\n",
    "       AdaBoostClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install mlxtend\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "def plot_model(X, y, clf) :\n",
    "    clf.fit(X, y)        # 예측하기\n",
    "    accuracy =       # 5겹 교차검증하여 평균 정확도 구하기\n",
    "    y_predict =                                # 예측 구하기\n",
    "    y_proba_buy =            # 구매 확률 구하기\n",
    "    print(f'모델 {clf}의 정확도 {accuracy}, 예측분류값{y_predict}, 예측확률{y_proba_buy}')\n",
    "    print('*'*100)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plot_decision_regions(X, y, clf=clf, legend=2)\n",
    "    plt.title(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for clf in MLA :\n",
    "    plot_model(X, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 실습_04 예측 알고리즘 적용\n",
    "    \n",
    "- 'encoded_data_TV_선호_모두_1'을 불러와 MLA를 적용하여 각 모델의 분석보고서를 각각 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/encoded_data_TV_선호_모두_1.xlsx', index_col=0)\n",
    "df.head()\n",
    "X = df.drop(['구매'], axis=1)\n",
    "y = df['구매']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=11) # 훈련과 검증데이터 8:2로 구분하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score \n",
    "from sklearn.metrics import average_precision_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for model in MLA:\n",
    "    print('모델은', type(model))\n",
    "    model.fit() # 훈련데이터 학습하기\n",
    "    y_pred =   # 예측하기\n",
    "    y_proba =  # 예측확률 구하기\n",
    "    y_score =  # 양성 확률만 구하기\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    confusion_matrix(y_test, y_pred)\n",
    "    result = {\n",
    "        'AUROC' : roc_auc_score(y_test, y_score),\n",
    "        'Average PR' : average_precision_score(y_test, y_score),\n",
    "        'Log_loss' : log_loss(y_test, y_proba)\n",
    "    }\n",
    "    print('*'*50)\n",
    "    \n",
    "    results.update({type(model): result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 실습_05_   당뇨 자료를 읽어 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "X = pd.DataFrame(load_diabetes().data, columns=load_diabetes().feature_names)\n",
    "y = load_diabetes().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, precision_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import f1_score, recall_score, log_loss, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "classifiers.update({'LGR': LogisticRegression(random_state=11, solver ='liblinear')})\n",
    "classifiers.update({'SVC': SVC(random_state=11, max_iter=300, probability=True)})\n",
    "# 랜덤포레스트 정의\n",
    "# 그레디언트 부스트 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECISION_FUNCTIONS = {'SVC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters.update({'LGR': \n",
    "{ \n",
    "'classifier__C': [0.01, 0.1, 1, 10], 'classifier__penalty': ['l1', 'l2']}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.update({'SVC': \n",
    "{ \n",
    "'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "'classifier__gamma': ['auto'],\n",
    "'classifier__C': [0.1, 0.5, 1, 5, 10, 50, 100],\n",
    "'classifier__degree': [1, 2, 3, 4, 5, 6]\n",
    "}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.update({'Random Forest': \n",
    "{ \n",
    "'classifier__n_estimators': [200],\n",
    "'classifier__class_weight': [None, 'balanced'],\n",
    "'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "'classifier__max_depth' : [3, 4, 5, 6, 7, 8],\n",
    "'classifier__min_samples_split': [0.005, 0.01, 0.05, 0.10],\n",
    "'classifier__min_samples_leaf': [0.005, 0.01, 0.05, 0.10],\n",
    "'classifier__criterion' :['gini', 'entropy']     ,\n",
    "'classifier__n_jobs': [-1]\n",
    "}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.update({'Gradient Boosting': \n",
    "{ \n",
    "'classifier__learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001], \n",
    "'classifier__n_estimators': [200],\n",
    "'classifier__max_depth': [2,3,4,5,6],\n",
    "'classifier__min_samples_split': [0.005, 0.01, 0.05, 0.10],\n",
    "'classifier__min_samples_leaf': [0.005, 0.01, 0.05, 0.10],\n",
    "'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "'classifier__subsample': [0.8, 0.9, 1]\n",
    "}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "kf = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for classifier_label, classifier in tqdm_notebook(classifiers.items()):\n",
    "    steps = [('scaler', StandardScaler()), ('classifier', classifier)]\n",
    "    pipeline = Pipeline(steps = steps)\n",
    "    param_grid = parameters[classifier_label]\n",
    "    \n",
    "    gs = RandomizedSearchCV(pipeline, param_grid, cv = 2,  n_iter = 2,\n",
    "                            scoring = 'recall_macro')    \n",
    "\n",
    "\n",
    "    y_pred = gs.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    if classifier_label in DECISION_FUNCTIONS:\n",
    "        y_proba = gs.decision_function(X_test)\n",
    "        ap = average_precision_score(y_test, y_proba)\n",
    "    else:\n",
    "        y_proba = gs.predict_proba(X_test)[:,1]\n",
    "        ap = average_precision_score(y_test, y_proba)\n",
    "\n",
    "   \n",
    "    result = {'Best Parameters': gs.best_params_,\n",
    "              'Training recall_macro': gs.best_score_,\n",
    "              'AUROC': roc_auc_score(y_test, y_proba),\n",
    "              'Average PR': ap,\n",
    "              'F1-score': f1_score(y_test, y_pred),\n",
    "              'CV accuracy': cross_val_score(gs, X_train, y_train).mean(),\n",
    "              'Log_loss': log_loss(y_test, y_proba),\n",
    "             }\n",
    "    \n",
    "    results.update({classifier_label: result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).T.sort_values(by = 'F1-score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 실습_06_   모델 해석(Model Interpretability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/encoded_data_TV_선호_scaled.xlsx', index_col=0)\n",
    "df = df.sample(frac=0.1)\n",
    "X = df.drop(['구매'], axis=1)\n",
    "y = df['구매']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=11) # 훈련과 검증데이터 8:2로 구분하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## □ 특성변수 model-based 중요도\n",
    "\n",
    "랜덤포레스트 모델의 입력변수의 중요도를 X축으로 하여 변수명을 그림으로 나타내었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.rcParams['font.family']='Malgun Gothic' # 한글폰트\n",
    "def plot_feature_importances(model):\n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "    plt.barh(range(X_train.shape[1]), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(X_train.shape[1]), X_train.columns)\n",
    "    plt.xlabel('변수의 중요도')\n",
    "    plt.ylabel('변수')\n",
    "plot_feature_importances(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## □ 특성변수 permutation-based 중요도\n",
    "\n",
    "특정 변수의 값을 무작위로 재정렬하여(permutation) 학습하는 경우 모델의 정확도에 영향을 얼마나 주는지를 가리키는 지표로 앞의 ‘model_based’ 중요도의 크기와는 다르다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(model, X_train, y_train, n_repeats=10, random_state=0)\n",
    "result.importances_mean.round(4)\n",
    "np.argsort(result.importances_mean)\n",
    "data = pd.DataFrame(list(zip(X_train.columns, result.importances_mean)))\n",
    "data.sort_values(by =1, ascending= False, inplace =True)\n",
    "data.plot(kind='bar', x=0, y=1, rot='60', figsize=(25, 6), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## □ 특성변수 SHAP value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install shap\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(model) \n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 실습_07_ low code (pycaret)\n",
    "https://pycaret.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#! pip install pycaret\n",
    "from pycaret.utils import version\n",
    "version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = setup(data = diabetes, target='Class variable', silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = create_model('lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunes_lr = tune_model(lr, fold=3)\n",
    "tunes_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(tunes_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_holdout = predict_model(tunes_lr)\n",
    "pred_holdout.shape\n",
    "pred_holdout.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
